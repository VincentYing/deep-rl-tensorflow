\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

%\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{bbm}

\title{Deep Q-Learning with Recurrent Neural Networks}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Clare Chen \\
  \texttt{cchen9@stanford.edu} \\
  \And
  Vincent Ying \\
  \texttt{vincenthying@stanford.edu} \\
  \And
  Dillon Laird \\
  \texttt{dalaird@cs.stanford.edu} \\
}

% Remove NIPS footer
\pagestyle{empty}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Deep reinforcement learning models have proven to be successful at learning
  control policies image inputs. They have, however, struggled with learning
  policies that require longer term information. Recurrent neural network
  architectures have be used in tasks dealing with longer term dependencies
  between data points. We investigate these architectures to overcome the
  difficulties arising from learning policies with long term dependencies.
\end{abstract}

\section{Introduction}
    Recent advances in reinforcement Learning have led to human-level or greater
    performance on a wide variety of games (e.g. Atari 2600 Games).  However,
    training these networks can take a long time, and the techniques presented in
    the state of the art [0] perform poorly on several games that require long
    term planning.  Deep Q-networks learn to estimate the Q-values (long term
    discounted returns) of selecting each possible action from the current game
    state. \\
    \\
    Deep Q-networks are limited in that they learn a mapping from a
    limited number of past states, or game screens.  In practice, DQN is trained
    using an input consisting of the last four game screens.  Thus, DQN performs
    poorly at games that require the agent to remember information more than four
    screens ago.  In other words, the game could no longer be modeled as a true
    Markov decision process; all of the information needed to make an optimal
    action would no longer be contained in a single state. This is evident from
    types of games DQN performs poorly at, near or below human-level Figure 1. We
    explore the concept of a deep recurrent Q-network, a combination of a long
    short term memory (LSTM) [6] and a deep recurrent Q-network (DRQN) similar to [5].  We
    wish to demonstrate with the introduction of recurrent network architecture into the
    deep Q-network, the network can retain information from previous frames of the
    game and achieve good performance on games that require long term planning. \\

    \begin{figure}[h]
        \centering
        \begin{minipage}{0.8\textwidth}
            \centering
            \includegraphics[scale=0.15]{Qbert}
            \centering
            \includegraphics[scale=0.15]{MsPacman}
            \centering
            \includegraphics[scale=0.15]{MontezumaRevenge}
        \end{minipage}
        \caption{Q*bert, Ms. Pac-Man and Montezuma's Revenge}
    \end{figure}

    In addition, recent achievements of attention recurrent neural networks in
    long term sequence tasks [2, 3] have introduced the possibility of incorporating
    them into the structure of the DRQN architecture.  The advantage of using
    attention is that it enables DRQN to focus on particular previous states it
    deems important for predicting the action in the current state. We investigate
    augmenting DRQN with attention and evaluate its usefulness. \\


\section{Deep Recurrent Q-Learning}
We propose two architectures for the Deep Recurrent Q-Network (DRQN). The first
is a very basic extension of DQN. We look at the last $L$ states, $\{s_{t-(L-1)},
\dots, s_{t}\}$ and feed these into a convolutional neural network (CNN) to get
intermediate outputs $\text{CNN}(s_{t-i}) = x_{t-i}$. These are then fed into a
RNN (we use an LSTM for this but it can be any RNN), $\text{RNN}(x_{t-i}) =
h_{t-i}$, and the final output $h_t$ is used to predict the $Q$ value.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{RDQN}
    \caption{Architecture of the DRQN}
\end{figure}

Another architecture we used was a version of an attention RNN. For the attention
RNN we take the $L$ hidden states outputted by the RNN, $\{h_{t-(L-1)}, \dots, h_{t}\}$
and we take the inner product with a learned vector $v_a$, $\{v_a^Th_{t-(L-1)}, \dots,
v_a^Th_{t}\}$. We then take a softmax over these values, $a_{t-i} =
\text{softmax}(v_a^Th_{t-i})$. We use this softmax to take a weighted sum over
the hidden states to get a context vector, $c_t = \sum_{i=0}^{L-1}a_{t-i}h_{t-i}$.
This context vector is then used to predict the $Q$ value. \\
\\
Learning sequences of observations creates some difficulty when sampling experiences
and using the following loss function [0]:
$$L_i(\theta_i) = \mathbb{E}_{e_t \sim U(D)}
\Big[\Big(
r_t + \gamma \max_{a_t'}Q(s_{t+1}, a_t'; \theta_i^-) -Q(s_t, a_t; \theta_i)
\Big)^2 \Big]$$
where $e_t = (s_t, a_t, r_t, s_{t+1})$, $D_t = \{e_1, \dots, e_t\}$, $\gamma$
is the discount factor and $e_t \sim U(D)$ are drawn uniformly at random from the
pool of stored experiences. We propose a simple solution where we sample $e_t
\sim U(D)$ and then take the previous $L$ states, $\{s_{t-(L+1)}, \dots, s_t\}$
and zero out states from previous games. For example if $s_{t-i}$ was the end of
the previous game then we would have states $\{\mathbf{0}, \dots, \mathbf{0},
s_{t-(i+1)}, \dots, s_t\}$ and similarly for the next state $\{\mathbf{0}, \dots,
\mathbf{0},s_{t-(i+1)}, \dots, s_{t+1}\}$.


\section{Experiments}
For our experiments, we chose to play the game Q*bert. We chose Q*bert because it
is a challenging game for DQN, which achieved scores only slightly above human-level
[0]. However, it wasn't so challenging that DQN could not make any progress, versus a game such as
Montezuma's Revenge [0]. \\
\\ 
For input, the RNN-CNN network takes 4 $80 \times 80$ preprocessed game screen images. (We will experiment with a reduced state space of 3 or 2 game screens in the future to show that reducing the state space does not degrade performance). The first hidden layer convolves 16 $19 \times 19$ filters with stride 8 across the input image and applies a rectifier nonlinearly.  The second hidden layer convolves 32 $8 \times 8$ filters with stride 4, again followed by a rectifier nonlinearily.  Convolutional outputs are fed to the fully connected LSTM layer.  Finally, a fully connected linear layer outputs a Q-value for each possible action. \\
\\
Episodes are selected randomly from the replay memory and updates begin at the beginning of the episode and proceed forward through time to the conclusion of the episode. The targets at each time step are generated from the target Q-network.  The RNN?s hidden state is carried forward throughout the episode.  \\


\section*{References}
\small
% TODO: need to figure out how to cite this stuff properly..
[0] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level Control through Deep Reinforcement Learning. {\it Nature}, 518(7540):529-522, 2015.

[1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra and Martin Riedmiller. Playing Atari with Deep Reinforcement Learning. {\it arXiv preprint arXiv:1312.5602}, 2013.

[2] Dzmitry Bahdanau, Kyunghyun Cho and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. {\it arXiv preprint arXiv:1409.0473}, 2014.

[3] Minh-Thang Luong, Hieu Pham and Chisopher D. Manning. Effective Approaches to Attention-based Neural Machine Translation. {\it arXiv preprint arXiv:1508.04025}, 2015.

[4] Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov and Anastasiia Ignateva. Deep Attention Recurrent Q-Network. {\it arXiv preprint arXiv:1512.01693}, 2015.

[5] Matthew Hausknecht and Peter Stone. Deep Recurrent Q-Learning for Partially Observable MDPs. {\it arXiv preprint arXiv:1507.06527}, 2015.

[6] Sepp Hochreiter and Jurgen Schmidhuber. Long Short-Term Memory. {\it Neural Computation}, 9(8):1735-1780, 1997.

\end{document}
