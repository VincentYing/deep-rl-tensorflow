\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

%\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{bbm}

\title{Deep Q-Learning with Recurrent Neural Networks}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Clare Chen \\
  \texttt{cchen9@stanford.edu} \\
  \And
  Vincent Ying \\
  \texttt{vincenthying@stanford.edu} \\
  \And
  Dillon Laird \\
  \texttt{dalaird@cs.stanford.edu} \\
}

% Remove NIPS footer
\pagestyle{empty}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Deep reinforcement learning models have proven to be successful at learning
  control policies image inputs. They have, however, struggled with learning
  policies that require longer term information. Recurrent neural network
  architectures have be used in tasks dealing with longer term dependencies
  between data points. We investigate these architectures to overcome the
  difficulties arising from learning policies with long term dependencies.
\end{abstract}

\section{Introduction}
    Recent advances in Reinforcement Learning have led to human-level or greater
    performance on a wide variety of games (e.g. Atari 2600 Games).  However,
    training these networks can take a long time, and the techniques presented in
    the state of the art [add reference] perform poorly on several games that
    require long term planning.  Deep Q-Networks learn to estimate the Q-Values
    (long term discounted returns) of selecting each possible action from the
    current game state. \\
    \\
    Deep Q-networks are limited in the sense that they learn a mapping from a
    limited number of past states, or game screens.  In practice, DQN is trained
    using an input consisting of the last four game screens.  Thus, DQN performs
    poorly at games that require the agent to remember information more than four
    screens ago.  In other words, the game could no longer be modeled as a true
    Markov decision process; all of the information needed to make an optimal
    action would no longer be contained in a single state. This is evident from
    types of games DQN performs poorly at, near or below human-level Figure 1. We
    explore the concept of a Deep Recurrent Q-network, a combination of a Long
    Short Term Memory (LSTM) [add reference] and a Deep Q-network (DRQN).  We wish
    to demonstrate that introducing recurrent network architecture into the Deep
    Q-network, the network can retain information from previous frames of the
    game and achieve good performance on games that require long term planning. \\

    \begin{figure}[h]
        \centering
        \begin{minipage}{0.8\textwidth}
            \centering
            \includegraphics[scale=0.15]{Qbert}
            \centering
            \includegraphics[scale=0.15]{MsPacman}
            \centering
            \includegraphics[scale=0.15]{MontezumaRevenge}
        \end{minipage}
        \caption{Q*bert, Ms. Pac-Man and Montezuma's Revenge}
    \end{figure}

    In addition, recent achievements of attention recurrent neural networks in
    long term sequence tasks have introduced the possibility of incorporating
    them into the structure of teh DRQN algorithm.  The advantage of using
    attention is that it enables DRQN to focus on particular previous states it
    deems important for predicting the action in the current state. We investigate
    augmenting DRQN with attention and evaluate its usefulness. \\


\section{Deep Recurrent Q-Learning}
We propose two architectures for the Deep Recurrent Q-Network (DRQN). The first
is a very basic extension of DQN. We look at the last $L$ states, $\{s_{t-(L-1)},
\dots, s_{t}\}$ and feed these into a convolutional neural network (CNN) to get
intermediate outputs $\text{CNN}(s_{t-i}) = x_{t-i}$. These are then fed into a
RNN, $\text{RNN}(x_{t-i}) = h_{t-i}$, and the final output $h_t$ is used to
predict the $Q$ value.

\begin{figure}[h]
    \centering
    \includegraphics[scale=0.5]{RDQN}
    \caption{Architecutre of the RDQN}
\end{figure}

Another architecture we used was a version of an attention RNN. For the attention
RNN we take the $L$ hidden states outputted by the RNN, $\{h_{t-(L-1)}, \dots, h_{t}\}$
and we take the inner product with a learned vector $v_a$, $\{v_a^Th_{t-(L-1)}, \dots,
v_a^Th_{t}\}$. We then take a softmax over these values, $a_{t-i} =
\text{softmax}(v_a^Th_{t-i})$. We use this softmax to take a weighted sum over
the hidden states to get a context vector, $c_t = \sum_{i=0}^{L-1}a_{t-i}h_{t-i}$.
This context vector is then used to predict the $Q$ value. \\
\\
Learning sequences of observations creates some difficulty when sampling experiences
and using the following loss function [0]:
$$L_i(\theta_i) = \mathbb{E}_{e_t \sim U(D)}
\Big[\Big(
r_t + \gamma \max_{a_t'}Q(s_{t+1}, a_t'; \theta_i^-) -Q(s_t, a_t; \theta_i)
\Big)^2 \Big]$$
where $e_t = (s_t, a_t, r_t, s_{t+1})$, $D_t = \{e_1, \dots, e_t\}$, $\gamma$
is the discount factor and $e_t \sim U(D)$ are drawn uniformly at random from the
pool of stored experiences. We propose a simple solution where we sample $e_t
\sim U(D)$ and then take the previous $L$ states, $\{s_{t-(L+1)}, \dots, s_t\}$
and zero out states from previous games. For example if $s_{t-i}$ was the end of
the previous games then we would have states $\{\mathbf{0}, \dots, \mathbf{0},
s_{t-(i+1)}, \dots, s_t\}$ and similarly for the next state $\{\mathbf{0}, \dots,
\mathbf{0},s_{t-(i+1)}, \dots, s_{t+1}\}$. 
\section{Experiments}
For our experiments we chose to play the game Q*bert. We chose Q*bert because it
was a challenging game for DQN which achieved scores only slightly above human-level
[0] but it wasn't so challenging that DQN could not make any progress, such as
Montezuma's Revenge [0]. To preprocess the images we grayscale them and resize
them to $80\times 80$. \\
\\

\section*{References}
\small
% TODO: need to figure out how to cite this stuff properly..
[0] "Human-level control through deep reinforcement learning" Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg \& Demis Hassabis \\

[1] "Playing Atari with Deep Reinforcement Learning" Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller \\

[2] "Neural Machine Translation By Jointly Learning To Align and Translate" Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio \\

[3] "Deep Recurrent Q-Learning for Partially Observable MDPs" Matthew Hausknecht and Peter Stone \\

[4] "Prioritized Experience Replay" Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver \\

[5] "Effective Approaches to Attention-based Neural Machine Translation" Minh-Thang Luong, Hieu Pham, and Christopher D. Manning \\

[6] "Neural Machine Translation By Jointly Learning To Align And Translate" Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio \\

[7] "Deep Attention Recurrent Q-Network" Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov, and Anastasiia Ignateva \\

\end{document}
