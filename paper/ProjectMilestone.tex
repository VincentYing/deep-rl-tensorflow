\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2016
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2016}

%\usepackage[final]{nips_2016}

% to compile a camera-ready version, add the [final] option, e.g.:
\usepackage[final]{nips_2016}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Deep Q-Learning with Recurrent Neural Networks}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Clare Chen \\
  \texttt{cchen9@stanford.edu} \\
  \And
  Vincent Ying \\
  \texttt{vincenthying@stanford.edu} \\
  \And
  Dillon Laird \\
  \texttt{dalaird@cs.stanford.edu} \\
}

% Remove NIPS footer
\pagestyle{empty}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  Deep reinforcement learning models have proven to be successful at learning
  control policies image inputs. They have, however, struggled with learning
  policies that require longer term information. Recurrent neural network
  architectures have be used in tasks dealing with longer term dependencies
  between data points. We investigate these architectures to overcome the
  difficulties arising from learning policies with long term dependencies.
\end{abstract}

\section{Introduction}
Deep Q-Networks (DQNs) have had success playing Atari 2600 games [0] but have 
struggled with particular games involving policies that require longer term
information. This is evident from the types of games that DQNs perform poorly at,
near or below human-level, which includes Q*bert, Ms. Pac-Man, and in an extreme
case Montezuma's Revenge.
\begin{figure}[h]
    \centering
    \caption{Q*bert, Ms. Pac-Man and Montezuma's Revenge}
\end{figure}

Recurrent neural networks (RNNs) have been used in modeling longer sequences and
is one way to overcome DQNs difficulties with learning policies that require longer
term information. We investigate augmenting DQN architecture proposed in [0], utilizing
a convolutional neural network (CNN), with an RNN.
\section{Recurrent Deep Q-Learning}
DQN
\section{Experiments}
Experiments.
\section*{References}
\small

% TODO: need to figure out how to cite this stuff properly..
[0] "Human-level control through deep reinforcement learning" Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg \& Demis Hassabis

[1] "Playing Atari with Deep Reinforcement Learning" Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller \\

[2] "Model-Free Episodic Control" Charlse Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Runderman, Joel Z Leibo, Jack Rae, Daan Wierstra, Demis Hassabis \\

[3] "Neural Machine Translation By Jointly Learning To Align and Translate" Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio \\

[4] "Deep Recurrent Q-Learning for Partially Observable MDPs" Matthew Hausknecht and Peter Stone

\end{document}
